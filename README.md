# Machine Learning Notebooks
Helpful noteboks that I compiled while learning Machine Learning and Deep Learning from various sources on the Internet. 

## NumPy Basics:
1. [NumPy Basics](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/00.%20NumPy%20Basics/1.%20NumPy%20Basics.ipynb)

## Data Preprocessing:
1. [Feature Selection](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/1.%20Feature%20Selection.ipynb): Imputing missing values, Encoding, Binarizing.  

2. [Feature Scaling](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/2.%20Scaling%2C%20Normalizing.ipynb): Min-Max Scaling, Normalizing, Standardizing. 

3. [Feature Extraction](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/01.%20Data%20Preprocessing/3.%20Feature%20Extraction.ipynb): CountVectorizer, DictVectorizer, TfidfVectorizer. 

## Regression
1. Linear & Multiple Regression

    * a. [Theory and Derivation](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1A.%20Linear%20Regression%20and%20Gradient%20Descent%28Theory%29.ipynb)
    
    * b. [Linear Regression from scratch](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1B.%20Linear%20Regression%20and%20Gradient%20Descent%20.ipynb)
    
    * c. [Linear Regression using Scikit-learn](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/1C.%20Simple%20and%20Multiple%20Regression%20using%20Sci-kit%20learn.ipynb): Assumptions in Linear Regression, Dummy Variable Trap, Multivariable Regression. 

2. [Backward Elimination](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/2.%20Backward%20Elimination.ipynb): Method of Backward Elimination, P-values.

3. [Polynomial Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/3.%20Polynomial%20Regression.ipynb)

4. [Support Vector Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/4.%20Support%20Vector%20Regression.ipynb)

5. [Decision Tree Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/5.%20Decision%20Tree%20Regression.ipynb)

6. [Random Forest Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/6.%20Random%20Forest.ipynb)

7. [Robust Regression using Theil-Sen Regression](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/8.%20Robust%20Regression%20(TheilSen%20Regressor).ipynb)

8. [Pipelines in Scikit-Learn](http://nbviewer.jupyter.org/github/maykulkarni/Machine-Learning-Notebooks/blob/master/02.%20Regression/9.%20Pipelines%20in%20Sklearn.ipynb)

Sources/References list:
1. Machine Learning by Andrew Ng (Coursera)
2. Machine Learning A-Z (Udemy)
3. Deep Learning A-Z (Udemy)
4. Neural Networks by Geoffrey (Hinton Coursera)
5. Scikit-learn Cookbook (Second Edition) - Julian Avila
